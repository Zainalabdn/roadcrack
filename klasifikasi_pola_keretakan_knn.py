# -*- coding: utf-8 -*-
"""Klasifikasi_pola_keretakan KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oCnfPFh1mHJAqstq4c8qv3XaBK7qrXR9
"""

# Setup: Instalasi library versi tertentu agar kompatibel
!pip uninstall -y numpy scikit-image
!pip install numpy==1.23.5 scikit-image==0.19.3 joblib==1.3.2

from google.colab import drive
drive.mount('/content/drive')

"""### Import Library"""

import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.model_selection import GridSearchCV
from sklearn.decomposition import PCA
from sklearn.ensemble import VotingClassifier
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from skimage.feature import greycomatrix, greycoprops
from sklearn.utils import resample
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.neighbors import KNeighborsClassifier

"""### Data Preprocessing"""

dataset_path = "/content/drive/MyDrive/Dataset/Roadcrack"
classes = ['jalan_retak', 'jalan_berlubang', 'jalan_tidak_rusak']
image_size = (128, 128)

#Fungsi ekstraksi fitur GLCM dari gambar grayscale
def extract_glcm_features(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    distances = [1, 3, 5]
    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]

    glcm = greycomatrix(gray, distances=distances, angles=angles, symmetric=True, normed=True)

    features = []
    properties = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']

    for prop in properties:
        vals = greycoprops(glcm, prop)
        features.append(np.mean(vals))  # ambil rata-rata dari semua kombinasi

    return features

total_images = 0
for cls in classes:
    class_path = os.path.join(dataset_path, cls)
    num_images = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))])
    print(f"Jumlah gambar di kelas '{cls}': {num_images}")
    total_images += num_images

print(f"\nTotal seluruh gambar: {total_images}")

#Load dataset & ekstraksi fitur dari setiap gambar
X, y, image_paths = [], [], []

for label_index, label_name in enumerate(classes):
    folder = os.path.join(dataset_path, label_name)
    for file in os.listdir(folder):
        if file.endswith(('jpg', 'jpeg', 'png')):
            path = os.path.join(folder, file)
            img = cv2.imread(path)
            if img is not None:
                img = cv2.resize(img, image_size)
                features = extract_glcm_features(img)
                X.append(features)
                y.append(label_index)
                image_paths.append(path)

# Visualisasi persebaran kelas
label_names = [classes[i] for i in y]
df_label = pd.DataFrame({'Label': label_names})
plt.figure(figsize=(6,4))
sns.countplot(data=df_label, x='Label', order=classes, palette='Set2')
plt.title("Persebaran Jumlah Data Tiap Kelas")
plt.xlabel("Kelas Jalan")
plt.ylabel("Jumlah Gambar")
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

counts = Counter(y)
for idx, count in counts.items():
    print(f"{idx} ({classes[idx]}): {count} gambar")

"""### Oversampling"""

# Gabungkan fitur, label, dan path ke DataFrame
data = list(zip(X, y, image_paths))
df = pd.DataFrame(data, columns=['features', 'label', 'path'])

# Pisahkan berdasarkan kelas
df_0 = df[df['label'] == 0]
df_1 = df[df['label'] == 1]
df_2 = df[df['label'] == 2]

# Tentukan ukuran maksimum untuk oversampling
max_size = max(len(df_0), len(df_1), len(df_2))

# Oversample masing-masing kelas
df_0_upsampled = resample(df_0, replace=True, n_samples=max_size, random_state=42)
df_1_upsampled = resample(df_1, replace=True, n_samples=max_size, random_state=42)
df_2_upsampled = resample(df_2, replace=True, n_samples=max_size, random_state=42)

# Gabungkan dan shuffle
df_balanced = pd.concat([df_0_upsampled, df_1_upsampled, df_2_upsampled])
df_balanced = df_balanced.sample(frac=1, random_state=42)

# Kembalikan ke X, y, path
X = np.array(list(df_balanced['features']))
y = np.array(list(df_balanced['label']))
image_paths = list(df_balanced['path'])

# Visualisasi ulang persebaran kelas (setelah balancing)
balanced_label_names = [classes[i] for i in y]
df_balanced_label = pd.DataFrame({'Label': balanced_label_names})
plt.figure(figsize=(6,4))
sns.countplot(data=df_balanced_label, x='Label', order=classes, palette='Set1')
plt.title("Persebaran Setelah Oversampling")
plt.xlabel("Kelas Jalan")
plt.ylabel("Jumlah Gambar")
plt.tight_layout()
plt.show()

"""### Normalisasi"""

# Normalisasi fitur menggunakan StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

# (1) PCA untuk mengurangi noise dan mempertahankan fitur penting
pca = PCA(n_components=0.95)  # mempertahankan 95% variasi data
X_pca = pca.fit_transform(X)

"""### Cross-Validation"""

# --- Cross-Validation untuk K-Nearest Neighbors (KNN) ---
print("\n--- Cross-Validation untuk K-Nearest Neighbors (KNN) ---")
knn_model_cv = KNeighborsClassifier(n_neighbors=5) # Anda bisa eksperimen dengan n_neighbors
knn_scores = cross_val_score(knn_model_cv, X, y, cv=5)
print("Cross Validation Scores (5 Fold) for KNN:", knn_scores)
print("Rata-rata Akurasi KNN:", np.mean(knn_scores))
print("Standar Deviasi KNN:", np.std(knn_scores))

"""### Split & Train Data"""

X_train, X_test, y_train, y_test, X_train_paths, X_test_paths = train_test_split(
        X, y, image_paths, test_size=0.2, random_state=42)

# Split ulang dengan data hasil PCA
X_train, X_test, y_train, y_test = train_test_split(
    X_pca, y, test_size=0.2, random_state=42)

"""### Tuning

"""

# (2) Hyperparameter Tuning dengan Grid Search
param_grid = {
    'n_neighbors': list(range(1, 20)),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_knn.fit(X_train, y_train)

print("Best Parameters (KNN):", grid_knn.best_params_)
print("Best Cross-Val Score:", grid_knn.best_score_)

# --- Pelatihan dan Evaluasi K-Nearest Neighbors (KNN) ---
print("\n--- Hasil Klasifikasi K-Nearest Neighbors (KNN) ---")
knn_model = KNeighborsClassifier(n_neighbors=5) # Gunakan n_neighbors yang sama atau dioptimalkan
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)

print("Classification Report (KNN):")
print(classification_report(y_test, y_pred_knn, target_names=classes))
print(f"Accuracy (KNN): {accuracy_score(y_test, y_pred_knn):.2f}")

conf_mat_knn = confusion_matrix(y_test, y_pred_knn)
plt.figure(figsize=(7, 6)) # Ukuran plot bisa disesuaikan
sns.heatmap(conf_mat_knn, annot=True, xticklabels=classes, yticklabels=classes, cmap='Blues', fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (KNN)')
plt.show()

# (3) Evaluasi pada data uji setelah tuning
best_knn = grid_knn.best_estimator_
y_pred_best_knn = best_knn.predict(X_test)

print("\nClassification Report (Best KNN):")
print(classification_report(y_test, y_pred_best_knn, target_names=classes))
print(f"Accuracy (Best KNN): {accuracy_score(y_test, y_pred_best_knn):.2f}")

conf_mat_best_knn = confusion_matrix(y_test, y_pred_best_knn)
plt.figure(figsize=(7, 6))
sns.heatmap(conf_mat_best_knn, annot=True, xticklabels=classes, yticklabels=classes, cmap='Blues', fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Best KNN)')
plt.show()

# (4) Ensemble (Voting Classifier - Opsional)
voting = VotingClassifier(estimators=[
    ('knn', best_knn),
    ('nb', GaussianNB())
], voting='soft')

voting.fit(X_train, y_train)
y_pred_voting = voting.predict(X_test)

print("\nAccuracy (Voting Classifier):", accuracy_score(y_test, y_pred_voting))

"""### Visualisasi Hasil Prediksi"""

# (5) Visualisasi Hasil Prediksi KNN Terbaik (gunakan path asli jika tersedia)
ints_2_label = {0: 'jalan_retak', 1: 'jalan_lubang', 2: 'jalan_tidak_rusak'}
y_pred_to_visualize = y_pred_best_knn

random_selection = np.random.randint(0, len(y_test), 15)
fig = plt.figure(figsize=(20, 10))

for i, idx in enumerate(random_selection):
    # Ganti bagian ini jika image_paths tidak tersedia pasca PCA
    if 'X_test_paths' in locals() or 'X_test_paths' in globals():
        img = cv2.imread(X_test_paths[idx])
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    else:
        img = np.zeros((128, 128, 3), dtype=np.uint8)  # dummy image jika path tidak tersedia

    ax = fig.add_subplot(3, 5, i+1, xticks=[], yticks=[])
    ax.imshow(img)

    true_label = ints_2_label[y_test[idx]]
    pred_label = ints_2_label[y_pred_to_visualize[idx]]

    color = 'green' if y_test[idx] == y_pred_to_visualize[idx] else 'red'
    ax.set_title(f"True: {true_label}\nPredict: {pred_label}", color=color, fontsize=12)

plt.tight_layout()
plt.show()

"""### Save Model"""

model_to_save = best_knn
filename = 'best_knn_model.pkl'
joblib.dump(model_to_save, filename)
print(f"Model berhasil disimpan ke {filename}")

# Simpan model terbaik
joblib.dump(best_knn, 'best_knn_model.pkl')
# Simpan scaler juga
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(pca, 'pca.pkl')

print("Model dan scaler berhasil disimpan.")

"""### Deploy"""

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import cv2
# import numpy as np
# from skimage.feature import graycomatrix, graycoprops
# from PIL import Image
# from joblib import load
# 
# # Load model, scaler, dan PCA
# model = load("best_knn_model.pkl")
# scaler = load("scaler.pkl")
# pca = load("pca.pkl")
# 
# # Label kelas
# label_mapping = {
#     0: "Jalan Retak",
#     1: "Jalan Lubang",
#     2: "Jalan Tidak Rusak"
# }
# 
# # Ekstraksi fitur GLCM
# def extract_glcm_features(image):
#     gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
#     glcm = graycomatrix(gray, [1], [0], symmetric=True, normed=True)
#     contrast = graycoprops(glcm, 'contrast')[0, 0]
#     dissimilarity = graycoprops(glcm, 'dissimilarity')[0, 0]
#     homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]
#     energy = graycoprops(glcm, 'energy')[0, 0]
#     correlation = graycoprops(glcm, 'correlation')[0, 0]
#     ASM = graycoprops(glcm, 'ASM')[0, 0]
#     return [contrast, dissimilarity, homogeneity, energy, correlation, ASM]
# 
# # UI Strimlit
# st.set_page_config(page_title="Deteksi Kerusakan Jalan", page_icon="🛣️", layout="centered")
# st.markdown("<h4 style='text-align: center;'>Deteksi Kerusakan Jalan</h4>", unsafe_allow_html=True)
# st.caption("Upload gambar jalan untuk prediksi jenis kerusakannya.")
# 
# uploaded_file = st.file_uploader("Upload Gambar", type=["jpg", "jpeg", "png"])
# 
# if uploaded_file is not None:
#     image = Image.open(uploaded_file).convert("RGB")
#     st.image(image, caption=None, width=300)
#     img_cv = np.array(image)
#     features = extract_glcm_features(img_cv)
#     features_scaled = scaler.transform([features])
#     features_pca = pca.transform(features_scaled)
# 
#     prediction = model.predict(features_pca)[0]
#     proba = model.predict_proba(features_pca)[0]
# 
#     st.markdown("---")
#     st.markdown(f"<p style='font-size: 16px;'>🧠 <b>Prediksi:</b> {label_mapping[prediction]}</p>", unsafe_allow_html=True)
# 
#     st.markdown("<p style='margin-bottom: 4px;'>📊 <b>Probabilitas:</b></p>", unsafe_allow_html=True)
#     for i, p in enumerate(proba):
#         st.markdown(f"<small>{label_mapping[i]}: {p*100:.1f}%</small>", unsafe_allow_html=True)
#         st.progress(float(p))

from google.colab import files
files.download('best_knn_model.pkl')
files.download('scaler.pkl')
files.download('pca.pkl')

!pip install pyngrok

!ngrok authtoken 30b7u3DnmcV4B7Xj2EAoHPh3Phg_2XDTbP3VFR2KQRyBSmECG

!nohup streamlit run app.py &

from pyngrok import ngrok

# Disconnect semua tunnel aktif
ngrok.kill()

from pyngrok import ngrok
url = ngrok.connect(addr="http://localhost:8501")
url